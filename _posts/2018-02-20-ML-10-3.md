---
layout: post
title:  " Machine Learning 10-(3) ::  Dropout과 앙상블 "
date:   2018-02-20
excerpt: "  Dropout과 앙상블 "
cate : "post"
tag:
- Machine Learning
---

<iframe width="560" height="315" src="https://www.youtube.com/embed/wTxMsp22llc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>


---


## Solution for overfitting

{% capture images %}
/assets/img/machine_learning/ML_10_3_1.png
{% endcapture %}
{% include gallery images=images caption=" " cols=1 %} 


1. 학습 데이터를 늘리자.

2. features를 줄이면 되는데 굳이 할 필욘 없다.

3. `Regularization` : Let's not have too big numbers in the weight <br> 

Q. “Let's not have too big numbers in the weight”가 무슨 말이지 ?
{: .notice}

```

```

<br>

{% capture images %}
/assets/img/machine_learning/ML_10_3_2.png
{% endcapture %}
{% include gallery images=images caption=" " cols=1 %} 

* 각각 Element를 제곱한 값을 더한게 최소화가 되게 하자.


---


## Dropout

{% capture images %}
/assets/img/machine_learning/ML_10_3_3.png
{% endcapture %}
{% include gallery images=images caption=" " cols=1 %} 

Neural Networks에는 

`Solution for overfitting`에 대한 해결책이 

1가지 더 있다. 

바로 `Dropout`이다.

<br>

Random하게 어떤 neurons을 squeeze하자.

{% capture images %}
/assets/img/machine_learning/ML_10_3_4.png
{% endcapture %}
{% include gallery images=images caption=" " cols=1 %} 

<br>

그런데 Random하게 설정을 해도 될까? ...

가.능.하.다.

<center><b> How ? </b></center>

<br>

{% capture images %}
/assets/img/machine_learning/ML_10_3_5.png
{% endcapture %}
{% include gallery images=images caption=" " cols=1 %} 

각각의 neurons들은 

각각의 정보를 갖고 있다.

<br>

그래서

일부는 쉬게하고 

남은 것들로 훈련을 시킨다.

<br>

그리고 

마지막에 쉬게한 것들을 총 동원해서

예측을 한다.


---

## Dropout Implementation code

{% capture images %}
/assets/img/machine_learning/ML_10_3_6.png
{% endcapture %}
{% include gallery images=images caption=" " cols=1 %} 


주의할 점은 

학슬 할 때만 dropout을 한다.

실전에서는 dropout을 하지 않는다.


{% highlight python %}

# Train:
sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys, dropout_rate: 0.7})

# Evaluation:
print('Accuracy:', sess.run(accuracy, feed_dict={
      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))

{% endhighlight %}



---

## What is Ensemble ?


{% capture images %}
/assets/img/machine_learning/ML_10_3_7.png
{% endcapture %}
{% include gallery images=images caption=" " cols=1 %} 


Ensemble 앙상블이라 읽는다.

100개의 Data가 있을 때

1명한테 100번 학습을 시키는게 아니라

10명한테 10번 씩 학습을 시키는 것이다.

<br>

독립적으로 모델을 학습시키고

나중에 합치는 것이다.

실제로 정확도도 더 높게 나온다.

